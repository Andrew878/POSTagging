from nltk.corpus import brown
from nltk import FreqDist, WittenBellProbDist
from nltk import ngrams, bigrams
from nltk import tag, tokenize, accuracy
from math import log, exp
import time

import Viterbi as vit
import ErrorAnalysis as error
import BeamSearch
import BeamClass
import ForwardProbabilities
import BackwardProbabilities


def tag_all_sentences_viterbi(test_sents, freq_dist_tag_single, freq_dist_tag_single_SMOOTH,
                              freq_dist_tagWordPair_SMOOTH,
                              freq_dist_tagBigram_SMOOTH, freq_dist_tagWordPairWithStartEnd_SMOOTH):
    tag_estimated_by_viterbi = []

    for test_sent in test_sents:
        tag_sequence = vit.viterbi_path(test_sent, freq_dist_tag_single, freq_dist_tag_single_SMOOTH,
                                        freq_dist_tagWordPair_SMOOTH,
                                        freq_dist_tagBigram_SMOOTH, freq_dist_tagWordPairWithStartEnd_SMOOTH)
        tag_estimated_by_viterbi.append(tag_sequence)

    return tag_estimated_by_viterbi


def tag_all_sentences_beam(test_sents, freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag, beam_width):
    tag_estimated_by_beam = []
    # minus two to remove start and end tags
    beam_search_vit = BeamSearch.BeamSearch(freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag, beam_width)

    i = 0
    for test_sent in test_sents:
        # print ("Sent index", i)
        # print("words", test_sent)
        tag_sequence = beam_search_vit.viterbi_path(test_sent)
        tag_estimated_by_beam.append(tag_sequence)
        i += 1
    return tag_estimated_by_beam


def tag_all_sentences_forward_Back(test_sents, freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag):
    tag_estimated_by_beam = []

    forward_prob_calculator = ForwardProbabilities.ForwardProbabilities(freq_dist_tag_single, smoothed_tag_tag,
                                                                        smoothed_word_tag)
    backward_prob_calculator = BackwardProbabilities.BackwardProbabilities(freq_dist_tag_single, smoothed_tag_tag,

                                                                           smoothed_word_tag)
    f_prob, alpha = forward_prob_calculator.return_forward_prob(sent)
    b_prob, beta = backward_prob_calculator.return_backward_prob(sent)

    non_beg_or_end_states = list(freq_dist_tag_single.keys())
    non_beg_or_end_states.remove('<s>')
    non_beg_or_end_states.remove('</s>')
    tag_to_index_dict = dict.fromkeys(non_beg_or_end_states)

    i = 0
    for test_sent in test_sents:
        # print ("Sent index", i)
        # print("words", test_sent)
        tag_sequence = forward_and_backward_combined(test_sent, forward_prob_calculator, backward_prob_calculator,
                                                     non_beg_or_end_states)
        tag_estimated_by_beam.append(tag_sequence)
        i += 1
    return tag_estimated_by_beam


# start and end symbols
start = '<s>'
end = '</s>'
max_sentence_length = 101
sents = brown.tagged_sents(tagset='universal')
train_size = 2000
test_size = 500

# add start and end states and tokens to each sentence. Making sure sentences are not too long (to avoid underflow)
sents = [[(start, start)] + sent + [(end, end)] for sent in sents[:train_size + test_size] if len(sent) < max_sentence_length]

# split into test and training sets
train_sents = sents[:train_size]
test_sents = sents[train_size:train_size + test_size]

test_words_individual = []
test_tags_individual = []

freq_dist_tag_single = FreqDist()
freq_dist_tagWordPair = FreqDist()
freq_dist_tagWordPairWithStartEnd = FreqDist()
freq_dist_tagBigram = FreqDist()
freq_dist_tagBigramWithoutStartEnd = FreqDist()

bigrams_tags = []
j = 0

#-------------- process training set -----------------
# (1): we need a list of unique states/tags
for sent in train_sents:

    # split tag lists
    tag_list = [t for (_, t) in sent]
    freq_dist_tag_single += FreqDist(tag_list)
    bigrams_tags.append(bigrams(tag_list))


# (2): create a unique list of states
unique_tag_list = list(freq_dist_tag_single.keys())
unique_tag_list.remove(start)
unique_tag_list.remove(end)
un_smoothed = {}
smoothed_word_tag = {}
tags = set(unique_tag_list)

# (3): For each sentence count the word frequency for each tag
for sent in train_sents:
    for tag in tags:
        words = [w for (w, t) in sent if t == tag]

        if (not un_smoothed.has_key(tag)):
            un_smoothed[tag] = FreqDist(words)
        else:
            un_smoothed[tag] += FreqDist(words)

# (4): For each tag, find the smoothed probabilities using WittenBell
for tag in tags:
    smoothed_word_tag[tag] = WittenBellProbDist(un_smoothed[tag], bins=1e5)


# (5): repeat step (3) but for pairs of consecutive tags
un_smoothed = {}
smoothed_tag_tag = {}
tags.add(start)

# (6): for
for bigram_list in bigrams_tags:

    bigram_sent = list(bigram_list)
    for tag in tags:
        # print(tag)

        following_tag = [t2 for (t1, t2) in bigram_sent if t1 == tag]

        # print(following_tag)

        if (not un_smoothed.has_key(tag)):
            un_smoothed[tag] = FreqDist(following_tag)
        else:
            un_smoothed[tag] += FreqDist(following_tag)
for tag in tags:
    smoothed_tag_tag[tag] = WittenBellProbDist(un_smoothed[tag], bins=1e5)

# print('prob of NOUN -> VERB is', smoothed_tag_tag['NOUN'].prob('VERB'))
# print('prob of DET -> NOUN is', smoothed_tag_tag['DET'].prob('NOUN'))


j = 0
for sent in test_sents:
    word_list = [w for (w, _) in sent]
    tag_list = [t for (_, t) in sent]

    test_words_individual.append(word_list)
    test_tags_individual.append(tag_list)
    j += 1




test_words_to_tag = test_words_individual
test_sent_tags_act = test_tags_individual


# print("Sentence:", test_words_individual)
# print("tags:", test_tags_individual)

# all_test_sent_tags_est = tag_all_sentences_viterbi(test_words_to_tag, freq_dist_tag_single,
#                                                    freq_dist_tag_single_SMOOTH, freq_dist_tagWordPair_SMOOTH,
#                                                    freq_dist_tagBigram_SMOOTH, freq_dist_tagWordPairWithStartEnd_SMOOTH)


# forward_prob_calculator = ForwardProbabilities.ForwardProbabilities(freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag)
# f_prob, alpha = forward_prob_calculator.return_forward_prob(tokenize.word_tokenize("Hi, how are you?"))
#
# backward_prob_calculator = BackwardProbabilities.BackwardProbabilities(freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag)
#
# b_prob, beta = backward_prob_calculator.return_backward_prob(tokenize.word_tokenize("Hi, how are you?"))

def show(sents):
    for sent in sents:
        print sent


def forward_and_backward_combined(sent, forward_prob_calculator, backward_prob_calculator, non_beg_or_end_states):
    f_prob, alpha = forward_prob_calculator.return_forward_prob(sent)
    b_prob, beta = backward_prob_calculator.return_backward_prob(sent)
    sent.reverse()

    # print(sent)
    # print("alpha")
    # print(f_prob)
    # show(alpha)
    # print("beta")
    # print(b_prob)
    # show(beta)
    tag_estimated_by_forward_backward = []

    # word_index_alpha = 2 + 1
    # word_index_beta = len(sent) - word_index_alpha
    sentence_length = len(sent)
    for word_index in range(1, sentence_length - 1):

        word_index_alpha = word_index
        word_index_beta = (sentence_length - 1) - word_index_alpha

        max_prob = -10 ** 10
        max_prob_state = None
        for tested_state_index in range(0, len(non_beg_or_end_states)):

            # print("word_index_alpha",word_index_alpha)
            # print("word_index_beta",word_index_beta)
            #
            # print("----------------------------------- testing word ", sent[word_index_alpha], " state tested ", non_beg_or_end_states[tested_state_index])
            forward_prob_to_state = alpha[word_index_alpha][tested_state_index]
            back_prob_to_state = beta[word_index_beta][tested_state_index]
            # print("forward_prob ", forward_prob_to_state, "backward_prob ", back_prob_to_state,)
            prob_est = forward_prob_to_state + back_prob_to_state - smoothed_word_tag[
                non_beg_or_end_states[tested_state_index]].prob(sent[word_index_alpha])

            # print("max prob", max_prob)
            # print("max_prob_state ", non_beg_or_end_states[tested_state_index])
            # print("prob_est", prob_est)

            if (max_prob < prob_est):
                max_prob = prob_est
                max_prob_state = non_beg_or_end_states[tested_state_index]

            # print("max prob", max_prob)
            # print("max_prob_state ", non_beg_or_end_states[tested_state_index])

        # print("******returning max prob and max state********", max_prob, max_prob_state)
        tag_estimated_by_forward_backward.append(max_prob_state)

    # print(sent)
    # print("tag_estimated_by_forward_backward",tag_estimated_by_forward_backward)
    return tag_estimated_by_forward_backward


# test_sentence = ['<s>', 'how', 'are', 'you', '?', '</s>']

# forward_and_backward_combined(test_sentence, freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag)

# print(f_prob)
# print(exp(f_prob))
# print(b_prob)
# print(exp(b_prob))

time_start = time.time()
all_test_sent_tags_est_beam_all = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                         smoothed_tag_tag, smoothed_word_tag,
                                                         len(list(freq_dist_tag_single.keys())) - 2)
time_end = time.time()
beam_all_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_one = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                         smoothed_tag_tag, smoothed_word_tag, 1)
time_end = time.time()
beam_one_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_three = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                           smoothed_tag_tag, smoothed_word_tag, 3)
time_end = time.time()
beam_three_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_five = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                          smoothed_tag_tag, smoothed_word_tag, 5)
time_end = time.time()
beam_five_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_seven = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                           smoothed_tag_tag, smoothed_word_tag, 7)
time_end = time.time()
beam_seven_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_nine = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                          smoothed_tag_tag, smoothed_word_tag, 9)
time_end = time.time()
beam_nine_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_eleven = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                            smoothed_tag_tag, smoothed_word_tag, 11)
time_end = time.time()
beam_eleven_eleven = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_forward_back = tag_all_sentences_forward_Back(test_words_to_tag, freq_dist_tag_single,
                                                                     smoothed_tag_tag, smoothed_word_tag)
time_end = time.time()
ford_back_time = time_end - time_start






print("----- Testing Time -------")
min_sentence_size = 0
min_tag_diversity = 0
print("\ntrain_size ", train_size)
print("test_size ", test_size)
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_all, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_all" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_all_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_one, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_one" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_one_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_three, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_three" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_three_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_five, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_five" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_five_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_seven, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_seven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_seven_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_nine, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_nine" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_nine_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_eleven, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_eleven" + "," + str(
                                             min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(beam_eleven_eleven))
# #
# error.compare_two_approaches(test_sent_tags_act, all_test_sent_tags_est_beam_all, "beam all",
#                              all_test_sent_tags_est_beam_five, "beam 5")

print("----- Testing sentence size -------")
print("\ntrain_size ", train_size)
print("test_size ", test_size)
min_tag_diversity = 0

for min_sentence_size in range(10, 100, 10):
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_one, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_one" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_three, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_three" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_five, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_five" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_seven, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_seven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_nine, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_nine" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_eleven, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_eleven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_all, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_all" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))

print("----- Testing tag diversity -------")
print("\ntrain_size ", train_size)
print("test_size ", test_size)
min_sentence_size = 0

for min_tag_diversity in range(1, 13):
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_one, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_one" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_three, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_three" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_five, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_five" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_seven, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_seven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_nine, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_nine" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_eleven, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_eleven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_all, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_all" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))


print("----- Confusion Matrices -------\n")
print("Beam one")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_one, 0)
print("Beam three")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_three, 0)
print("Beam five")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_five, 0)
print("Beam seven")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_seven, 0)
print("Beam nine")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_nine, 0)
print("Beam eleven")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_eleven, 0)
print("Beam all")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_all, 0)
print("forward, backward")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_forward_back, 0)
