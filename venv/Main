from nltk.corpus import brown
from nltk import FreqDist, WittenBellProbDist
from nltk import ngrams, bigrams
from nltk import tag, tokenize, accuracy
from math import log, exp
import time

import Viterbi as vit
import ErrorAnalysis as error
import BeamSearch
import BeamClass
import ForwardProbabilities
import BackwardProbabilities


def tag_all_sentences_viterbi(test_sents, freq_dist_tag_single, freq_dist_tag_single_SMOOTH,
                              freq_dist_tagWordPair_SMOOTH,
                              freq_dist_tagBigram_SMOOTH, freq_dist_tagWordPairWithStartEnd_SMOOTH):
    tag_estimated_by_viterbi = []

    for test_sent in test_sents:
        tag_sequence = vit.viterbi_path(test_sent, freq_dist_tag_single, freq_dist_tag_single_SMOOTH,
                                        freq_dist_tagWordPair_SMOOTH,
                                        freq_dist_tagBigram_SMOOTH, freq_dist_tagWordPairWithStartEnd_SMOOTH)
        tag_estimated_by_viterbi.append(tag_sequence)

    return tag_estimated_by_viterbi


def tag_all_sentences_beam(test_sents, freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag, beam_width):
    tag_estimated_by_beam = []
    # minus two to remove start and end tags
    beam_search_vit = BeamSearch.BeamSearch(freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag, beam_width)

    i = 0
    for test_sent in test_sents:
        # print ("Sent index", i)
        # print("words", test_sent)
        tag_sequence = beam_search_vit.viterbi_path(test_sent)
        tag_estimated_by_beam.append(tag_sequence)
        i += 1
    return tag_estimated_by_beam


def tag_all_sentences_forward_Back(test_sents, freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag):
    tag_estimated_by_beam = []

    forward_prob_calculator = ForwardProbabilities.ForwardProbabilities(freq_dist_tag_single, smoothed_tag_tag,
                                                                        smoothed_word_tag)
    backward_prob_calculator = BackwardProbabilities.BackwardProbabilities(freq_dist_tag_single, smoothed_tag_tag,

                                                                           smoothed_word_tag)
    f_prob, alpha = forward_prob_calculator.return_forward_prob(sent)
    b_prob, beta = backward_prob_calculator.return_backward_prob(sent)

    non_beg_or_end_states = list(freq_dist_tag_single.keys())
    non_beg_or_end_states.remove('<s>')
    non_beg_or_end_states.remove('</s>')
    tag_to_index_dict = dict.fromkeys(non_beg_or_end_states)

    i = 0
    for test_sent in test_sents:
        # print ("Sent index", i)
        # print("words", test_sent)
        tag_sequence = forward_and_backward_combined(test_sent, forward_prob_calculator, backward_prob_calculator,
                                                     non_beg_or_end_states)
        tag_estimated_by_beam.append(tag_sequence)
        i += 1
    return tag_estimated_by_beam


start = '<s>'
end = '</s>'
max_sentence_length = 101
sents = brown.tagged_sents(tagset='universal')
train_size = 10000  # 100.
test_size = 1000

sents = [[(start, start)] + sent + [(end, end)] for sent in sents if len(sent) < max_sentence_length]

train_sents = sents[:train_size]
test_sents = sents[train_size:train_size + test_size]

train_words_individual = [[]] * len(train_sents)
train_tags_individual = [[]] * len(train_sents)
# test_words_individual = [[]] * len(test_sents)
# test_tags_individual = [[]] * len(test_sents)
test_words_individual = []
test_tags_individual = []

freq_dist_tag_single = FreqDist()
freq_dist_tagWordPair = FreqDist()
freq_dist_tagWordPairWithStartEnd = FreqDist()
freq_dist_tagBigram = FreqDist()
freq_dist_tagBigramWithoutStartEnd = FreqDist()

bigrams_tags = []
j = 0

for sent in train_sents:
    word_list = [w for (w, _) in sent]
    tag_list = [t for (_, t) in sent]
    tag_list_no_startEnd = [t for (_, t) in sent[1:-1]]

    freq_dist_tag_single += FreqDist(tag_list)
    freq_dist_tagWordPair += FreqDist(sent[1:-1])
    freq_dist_tagBigram += FreqDist(bigrams(tag_list))
    bigrams_tags.append(bigrams(tag_list))
    freq_dist_tagBigramWithoutStartEnd += FreqDist(bigrams(tag_list_no_startEnd))
    # skip the special start/end characters
    freq_dist_tagWordPairWithStartEnd += FreqDist(sent)

    train_words_individual[j].insert(0, word_list)
    train_tags_individual[j].insert(0, tag_list)
    j += 1

unique_tag_list = list(freq_dist_tag_single.keys())
unique_tag_list.remove(start)
unique_tag_list.remove(end)

un_smoothed = {}
smoothed_word_tag = {}
tags = set(unique_tag_list)

for sent in train_sents:
    for tag in tags:
        words = [w for (w, t) in sent if t == tag]

        if (not un_smoothed.has_key(tag)):
            un_smoothed[tag] = FreqDist(words)
        else:
            un_smoothed[tag] += FreqDist(words)
for tag in tags:
    smoothed_word_tag[tag] = WittenBellProbDist(un_smoothed[tag], bins=1e5)

# print('prob of NOUN -> change is', smoothed_word_tag['NOUN'].prob('change'))
# print('prob of DET -> the is', smoothed_word_tag['DET'].prob('the'))

un_smoothed = {}
smoothed_tag_tag = {}
tags.add(start)

# print(list(bigrams_tags[0]))

for bigram_list in bigrams_tags:

    bigram_sent = list(bigram_list)
    for tag in tags:
        # print(tag)

        following_tag = [t2 for (t1, t2) in bigram_sent if t1 == tag]

        # print(following_tag)

        if (not un_smoothed.has_key(tag)):
            un_smoothed[tag] = FreqDist(following_tag)
        else:
            un_smoothed[tag] += FreqDist(following_tag)
for tag in tags:
    smoothed_tag_tag[tag] = WittenBellProbDist(un_smoothed[tag], bins=1e5)

# print('prob of NOUN -> VERB is', smoothed_tag_tag['NOUN'].prob('VERB'))
# print('prob of DET -> NOUN is', smoothed_tag_tag['DET'].prob('NOUN'))


j = 0
for sent in test_sents:
    word_list = [w for (w, _) in sent]
    tag_list = [t for (_, t) in sent]

    test_words_individual.append(word_list)
    test_tags_individual.append(tag_list)
    j += 1

# print(train_words_individual[0][:])
# print(freq_dist_tag_single.most_common(15))
# print(freq_dist_tagWordPair.most_common(15))
# print(freq_dist_tagBigram.most_common(15))

print("")

freq_dist_tag_single_SMOOTH = WittenBellProbDist(freq_dist_tag_single, bins=1e5)
freq_dist_tagWordPair_SMOOTH = WittenBellProbDist(freq_dist_tagWordPair, bins=1e5)
freq_dist_tagWordPairWithStartEnd_SMOOTH = WittenBellProbDist(freq_dist_tagWordPairWithStartEnd, bins=1e5)
freq_dist_tagBigram_SMOOTH = WittenBellProbDist(freq_dist_tagBigram, bins=1e5)
freq_dist_tagBigramNoStartEnd_SMOOTH = WittenBellProbDist(freq_dist_tagBigramWithoutStartEnd, bins=1e5)

# print(freq_dist_tagWordPair_SMOOTH.prob(("change","NOUN")))
# print(freq_dist_tagWordPair_SMOOTH.prob(("the","DET")))


# test_sent_tags_est = vit.viterbi_path(test_words_individual[0], freq_dist_tag_single, freq_dist_tag_single_SMOOTH, freq_dist_tagWordPair_SMOOTH,
#                  freq_dist_tagBigram_SMOOTH, freq_dist_tagWordPairWithStartEnd_SMOOTH)

test_words_to_tag = test_words_individual
test_sent_tags_act = test_tags_individual


# print("Sentence:", test_words_individual)
# print("tags:", test_tags_individual)

# all_test_sent_tags_est = tag_all_sentences_viterbi(test_words_to_tag, freq_dist_tag_single,
#                                                    freq_dist_tag_single_SMOOTH, freq_dist_tagWordPair_SMOOTH,
#                                                    freq_dist_tagBigram_SMOOTH, freq_dist_tagWordPairWithStartEnd_SMOOTH)


# forward_prob_calculator = ForwardProbabilities.ForwardProbabilities(freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag)
# f_prob, alpha = forward_prob_calculator.return_forward_prob(tokenize.word_tokenize("Hi, how are you?"))
#
# backward_prob_calculator = BackwardProbabilities.BackwardProbabilities(freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag)
#
# b_prob, beta = backward_prob_calculator.return_backward_prob(tokenize.word_tokenize("Hi, how are you?"))

def show(sents):
    for sent in sents:
        print sent


def forward_and_backward_combined(sent, forward_prob_calculator, backward_prob_calculator, non_beg_or_end_states):
    f_prob, alpha = forward_prob_calculator.return_forward_prob(sent)
    b_prob, beta = backward_prob_calculator.return_backward_prob(sent)
    sent.reverse()

    # print(sent)
    # print("alpha")
    # print(f_prob)
    # show(alpha)
    # print("beta")
    # print(b_prob)
    # show(beta)
    tag_estimated_by_forward_backward = []

    # word_index_alpha = 2 + 1
    # word_index_beta = len(sent) - word_index_alpha
    sentence_length = len(sent)
    for word_index in range(1, sentence_length - 1):

        word_index_alpha = word_index
        word_index_beta = (sentence_length - 1) - word_index_alpha

        max_prob = -10 ** 10
        max_prob_state = None
        for tested_state_index in range(0, len(non_beg_or_end_states)):

            # print("word_index_alpha",word_index_alpha)
            # print("word_index_beta",word_index_beta)
            #
            # print("----------------------------------- testing word ", sent[word_index_alpha], " state tested ", non_beg_or_end_states[tested_state_index])
            forward_prob_to_state = alpha[word_index_alpha][tested_state_index]
            back_prob_to_state = beta[word_index_beta][tested_state_index]
            # print("forward_prob ", forward_prob_to_state, "backward_prob ", back_prob_to_state,)
            prob_est = forward_prob_to_state + back_prob_to_state - smoothed_word_tag[
                non_beg_or_end_states[tested_state_index]].prob(sent[word_index_alpha])

            # print("max prob", max_prob)
            # print("max_prob_state ", non_beg_or_end_states[tested_state_index])
            # print("prob_est", prob_est)

            if (max_prob < prob_est):
                max_prob = prob_est
                max_prob_state = non_beg_or_end_states[tested_state_index]

            # print("max prob", max_prob)
            # print("max_prob_state ", non_beg_or_end_states[tested_state_index])

        # print("******returning max prob and max state********", max_prob, max_prob_state)
        tag_estimated_by_forward_backward.append(max_prob_state)

    # print(sent)
    # print("tag_estimated_by_forward_backward",tag_estimated_by_forward_backward)
    return tag_estimated_by_forward_backward


# test_sentence = ['<s>', 'how', 'are', 'you', '?', '</s>']

# forward_and_backward_combined(test_sentence, freq_dist_tag_single, smoothed_tag_tag, smoothed_word_tag)

# print(f_prob)
# print(exp(f_prob))
# print(b_prob)
# print(exp(b_prob))

time_start = time.time()
all_test_sent_tags_est_beam_all = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                         smoothed_tag_tag, smoothed_word_tag,
                                                         len(list(freq_dist_tag_single.keys())) - 2)
time_end = time.time()
beam_all_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_one = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                         smoothed_tag_tag, smoothed_word_tag, 1)
time_end = time.time()
beam_one_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_three = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                           smoothed_tag_tag, smoothed_word_tag, 3)
time_end = time.time()
beam_three_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_five = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                          smoothed_tag_tag, smoothed_word_tag, 5)
time_end = time.time()
beam_five_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_seven = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                           smoothed_tag_tag, smoothed_word_tag, 7)
time_end = time.time()
beam_seven_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_nine = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                          smoothed_tag_tag, smoothed_word_tag, 9)
time_end = time.time()
beam_nine_time = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_beam_eleven = tag_all_sentences_beam(test_words_to_tag, freq_dist_tag_single,
                                                            smoothed_tag_tag, smoothed_word_tag, 11)
time_end = time.time()
beam_eleven_eleven = time_end - time_start

time_start = time.time()
all_test_sent_tags_est_forward_back = tag_all_sentences_forward_Back(test_words_to_tag, freq_dist_tag_single,
                                                                     smoothed_tag_tag, smoothed_word_tag)
time_end = time.time()
ford_back_time = time_end - time_start






print("----- Testing Time -------")
min_sentence_size = 0
min_tag_diversity = 0
print("\ntrain_size ", train_size)
print("test_size ", test_size)
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_all, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_all" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_all_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_one, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_one" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_one_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_three, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_three" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_three_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_five, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_five" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_five_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_seven, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_seven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_seven_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_nine, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_nine" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(
                                             beam_nine_time))
error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_eleven, min_sentence_size,min_tag_diversity,
                                         "all_test_sent_tags_est_beam_eleven" + "," + str(
                                             min_sentence_size)+ "," + str(min_tag_diversity) + "," + str(beam_eleven_eleven))
# #
# error.compare_two_approaches(test_sent_tags_act, all_test_sent_tags_est_beam_all, "beam all",
#                              all_test_sent_tags_est_beam_five, "beam 5")

print("----- Testing sentence size -------")
print("\ntrain_size ", train_size)
print("test_size ", test_size)
min_tag_diversity = 0

for min_sentence_size in range(10, 100, 10):
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_one, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_one" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_three, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_three" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_five, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_five" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_seven, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_seven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_nine, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_nine" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_eleven, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_eleven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_all, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_all" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))

print("----- Testing tag diversity -------")
print("\ntrain_size ", train_size)
print("test_size ", test_size)
min_sentence_size = 0

for min_tag_diversity in range(1, 13):
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_one, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_one" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_three, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_three" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_five, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_five" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_seven, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_seven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_nine, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_nine" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_eleven, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_eleven" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))
    error.compare_accuracy_for_sentence_size(test_sent_tags_act, all_test_sent_tags_est_beam_all, min_sentence_size,min_tag_diversity,
                                             "all_test_sent_tags_est_beam_all" + "," + str(min_sentence_size)+ "," + str(min_tag_diversity))


print("----- Confusion Matrices -------\n")
print("Beam one")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_one, 0)
print("Beam three")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_three, 0)
print("Beam five")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_five, 0)
print("Beam seven")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_seven, 0)
print("Beam nine")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_nine, 0)
print("Beam eleven")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_eleven, 0)
print("Beam all")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_beam_all, 0)
print("forward, backward")
error.get_confusion_matrix_for_sent_size(test_sent_tags_act, all_test_sent_tags_est_forward_back, 0)
